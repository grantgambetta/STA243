{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Implement GMM clustering on the MNIST Dataset using spherical and diagonal Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "test_datapath = 'data/t10k-images-idx3-ubyte'\n",
    "train_datapath = 'data/train-images-idx3-ubyte'\n",
    "test_labelpath = 'data/t10k-labels-idx1-ubyte'\n",
    "train_labelpath = 'data/train-labels-idx1-ubyte'\n",
    "\n",
    "# Reading the u-byte files and converting them into numpy arrays\n",
    "X_train = idx2numpy.convert_from_file(train_datapath)\n",
    "y_train = idx2numpy.convert_from_file(train_labelpath)\n",
    "\n",
    "X_test = idx2numpy.convert_from_file(test_datapath)\n",
    "y_test = idx2numpy.convert_from_file(test_labelpath)\n",
    "\n",
    "# Filtering the dataset to only include the digits 0-4\n",
    "train_idx = np.where((y_train == 0) | (y_train == 1) | \n",
    "                     (y_train == 2) | (y_train == 3) | (y_train == 4))\n",
    "\n",
    "test_idx = np.where((y_test == 0) | (y_test == 1) | \n",
    "                     (y_test == 2) | (y_test == 3) | (y_test == 4))\n",
    "\n",
    "# Subsetting the dataset\n",
    "X_train = X_train[train_idx]\n",
    "X_test = X_test[test_idx]\n",
    "y_train = y_train[train_idx]\n",
    "y_test = y_test[test_idx]\n",
    "\n",
    "# Normalizing the pixel values for easier computation\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressing the images into 14x14 pixels\n",
    "X_train_compressed = np.zeros((X_train.shape[0], 14,14))\n",
    "X_test_compressed = np.zeros((X_test.shape[0], 14,14))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_compressed[i] = np.add.reduceat(np.add.reduceat(X_train[i], np.arange(0,X_train[i].shape[0],2), axis=0)\n",
    "                                            , np.arange(0,X_train[i].shape[1],2), axis=1)/4\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_compressed[i] = np.add.reduceat(np.add.reduceat(X_test[i], np.arange(0,X_test[i].shape[0],2), axis=0)\n",
    "                                            , np.arange(0,X_test[i].shape[1],2), axis=1)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the dataset to be of the form (n, 14*14)\n",
    "X_train_reshaped = X_train_compressed.reshape(X_train_compressed.shape[0], X_train_compressed.shape[1]*X_train_compressed.shape[2])\n",
    "X_test_reshaped = X_test_compressed.reshape(X_test_compressed.shape[0], X_test_compressed.shape[1]*X_test_compressed.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k, cluster_size, spherical=False, init = True, mean = np.zeros(1), cov = np.zeros(1), seed  = np.random.randint(1,1000)):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    A custom function that generates a stack of multivariate normal distributions \n",
    "\n",
    "    Args:\n",
    "        k (int): Dimensions of the distributions to generate\n",
    "        cluster_size (int): Number of distributions to generate\n",
    "        spherical (bool, optional): Indicator for Spherical or Diagonal Gaussians. Defaults to False.\n",
    "        init (bool, optional): Indicator for whether its the first time the functions called or not. Defaults to True.\n",
    "        mean (list, optional): Mean vector if init = False. Defaults to np.zeros(1).\n",
    "        cov (list, optional): Cov vector if init = False. Defaults to np.zeros(1).\n",
    "        seed (int, optional): Random seed generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Stack of multivariate normal distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    # Check if its the first time the function is called\n",
    "    if init:\n",
    "        # Generating the mean vectors\n",
    "        means = np.array([np.random.rand(k) for _ in range(cluster_size)])\n",
    "        # Check if the distribution is spherical or diagonal\n",
    "        if spherical:\n",
    "            # Generating the covariance matrices  \n",
    "            covariance_matrix = [np.diag(np.ones(k)*np.random.rand()) for _ in range(cluster_size)]\n",
    "        else:\n",
    "            covariance_matrix = [np.diag(np.random.rand(k)) for _ in range(cluster_size)]\n",
    "        \n",
    "        # Creating the stack of multivariate normal distributions\n",
    "        normal_distributions = [multivariate_normal(\n",
    "            mean=means[i], cov=covariance_matrix[i]) for i in range(cluster_size)]\n",
    "    else:\n",
    "        # Creating the stack of multivariate normal distributions based on given values\n",
    "        normal_distributions = [multivariate_normal(mean = mean[i], cov = cov[i]) \n",
    "                                for i in range(cluster_size)]\n",
    "    \n",
    "    # Returning the stack of multivariate normal distributions\n",
    "    return np.array(normal_distributions)\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Q(theta^(t),theta)\n",
    "def log_likelihood(X, initial_distributions,pi, clusters):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    Function to generate values of Q(theta^(t),theta)\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        np.array: Q(theta^(t),theta) values for every cluster\n",
    "    \"\"\"\n",
    "    values = np.array([\n",
    "        initial_distributions[i].logpdf(X) + np.log(pi[i]) for i in range(clusters)\n",
    "    ])\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_likelihood(X, initial_distributions,pi, clusters):\n",
    "    \"\"\"Summary\n",
    "    Calculates the log likelihood of the data given the initial distributions\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        np.float64: Log likelihood of the data given the initial distributions\n",
    "    \"\"\"\n",
    "    values = np.sum([\n",
    "        initial_distributions[i].pdf(X)* pi[i] for i in range(clusters)\n",
    "    ])\n",
    "    \n",
    "    return np.log(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_step(X,initial_distributions, pi, k):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "\n",
    "    Function to emulate the expectation step of the EM algorithm\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        tuple: Stores the value of F and the cluster values for every x_i\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing the predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Calculating Q(theta^(t),theta)\n",
    "    log_likelihood_values = log_likelihood(X, initial_distributions, pi, k)\n",
    "    \n",
    "    # Initializing F \n",
    "    F = np.zeros_like(log_likelihood_values.T)\n",
    "    \n",
    "    # Parsing the log likelihood values \n",
    "    for i in range(log_likelihood_values.T.shape[0]):\n",
    "        # Calculating the value of F for a data point x_i \n",
    "        F[i,] = np.exp(log_likelihood_values[:,i] - logsumexp(log_likelihood_values[:,i]))\n",
    "        \n",
    "        # Seeing which cluster the data point belongs to\n",
    "        max_val = np.argmax(F[i,:])\n",
    "        \n",
    "        # Storing the cluster value for the data point\n",
    "        predictions.append(max_val)\n",
    "        \n",
    "    # Returning the value of F and the cluster values for every x_i\n",
    "    return (F, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(X, F, clusters):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    \n",
    "    Function to emulate the maximization step of the EM algorithm\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        F (np.ndarray): Value of F for every data point obtained from the expectation step\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        tuple: vector of means and covariance matrices and updated probabilities for every cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the means and covariance matrices\n",
    "    means = []\n",
    "    covs = []\n",
    "    pis = []\n",
    "\n",
    "    # Parsing the F values for a fixed cluster\n",
    "    for j in range(clusters):\n",
    "        # Calculating the denominator of the mean and covariance matrix computation \n",
    "        denominator = np.sum(F[:,j], axis=0)\n",
    "        \n",
    "        # Calculating the mean for the cluster and storing it\n",
    "        means.append(np.sum(X * F[:,j].reshape(len(X),1), axis=0)/denominator)\n",
    "        \n",
    "        # Calculating the covariance diagonal for the cluster and storing it\n",
    "        cov_diag = []\n",
    "        for i in range(X.shape[1]):\n",
    "            # Creating the centering of the expression\n",
    "            center = ((X[:,j]-means[j][i])**2).reshape(-1,1)\n",
    "            # Calculating the covariance diagonal for the cluster and storing it\n",
    "            sigma = (1/denominator) * np.dot(F[:,j].reshape(1, len(X)), center)\n",
    "            \n",
    "            # Adding the diagonal to the list and adding 0.05 to prevent underflow\n",
    "            cov_diag.append(sigma[0][0] + 0.05)\n",
    "        \n",
    "        # Storing the covariance diagonal for the cluster\n",
    "        covs.append(np.diag(cov_diag))\n",
    "        \n",
    "        # Calculating the probability of the cluster and storing it\n",
    "        pis.append(denominator/np.sum(F))\n",
    "    \n",
    "    # Returning the vector of means and covariance matrices and updated probabilities for every cluster\n",
    "    return (means, covs, pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(X, normal_distributions, pi, k, threshold = 0.0001, maxiters = 1000):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "\n",
    "    Function to emulate the expectation maximization algorithm \n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Data Matrix\n",
    "        normal_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        k (int): Number of clusters\n",
    "        threshold (float, optional): Threshold that reduces the effect of . Defaults to 0.0001.\n",
    "        maxiters (int, optional): Maximum iterations incase the difference of likelihood doesnt converge. Defaults to 1000.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: vector of predictions, iterations taken and trend of the differences of likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the difference of likelihood\n",
    "    diff = []\n",
    "    for i in range(maxiters):\n",
    "        # Calculating the value of log likelihood\n",
    "        # and the cluster values for every x_i with t^th iteration mean and covariance\n",
    "        t_1_ll = calc_log_likelihood(X, normal_distributions, pi, k)    \n",
    "        \n",
    "        # Running the expectation step\n",
    "        F,predictions = expectation_step(X, normal_distributions, pi, k)\n",
    "        # Running the maximization step\n",
    "        means, covs, pi = maximization_step(X, F, k)\n",
    "        \n",
    "        # Updating the normal distributions based on new values of means and covariance matrices \n",
    "        normal_distributions = generate_data(X.shape[1], k, mean = means, cov = covs, init = False)\n",
    "        \n",
    "        # Calculating the value of log likelihood\n",
    "        # and the cluster values for every x_i with t+1^th iteration mean and covariance\n",
    "        t_ll = calc_log_likelihood(X, normal_distributions, pi, k)\n",
    "        \n",
    "        # Calculating the difference of likelihood\n",
    "        t_diff = np.abs(t_ll - t_1_ll)\n",
    "        \n",
    "        # Storing the difference of likelihood\n",
    "        diff.append(t_diff)\n",
    "        \n",
    "        # Checking if the difference of likelihood is less than the threshold\n",
    "        if t_diff < threshold:\n",
    "            return (predictions,i,diff)\n",
    "    \n",
    "    # Returning the vector of predictions, iterations taken and trend of the differences of likelihood\n",
    "    return (predictions,maxiters,diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_train,predictions):\n",
    "    # Subsetting the data based on the predictions\n",
    "    idx_0 = np.where(np.array(predictions) == 0)\n",
    "    idx_1 = np.where(np.array(predictions) == 1)\n",
    "    idx_2 = np.where(np.array(predictions) == 2)\n",
    "    idx_3 = np.where(np.array(predictions) == 3)\n",
    "    idx_4 = np.where(np.array(predictions) == 4)\n",
    "\n",
    "    idxs = [idx_0, idx_1, idx_2, idx_3, idx_4]\n",
    "    label_map = dict()\n",
    "    for i,id in enumerate(idxs):\n",
    "        # Setting label to the cluster\n",
    "        label_map[i] = np.bincount(y_train[id]).argmax()\n",
    "\n",
    "    # Generating correct label\n",
    "    labels = [label_map[i] for i in predictions]\n",
    "\n",
    "    print(\"Accuracy: \", accuracy_score(y_train, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting intial parameters\n",
    "N = X_train.shape[0]\n",
    "k = 5 \n",
    "threshold = 0.0001 \n",
    "\n",
    "# Generating random probabilities adding to one\n",
    "pi = np.random.dirichlet(np.ones(k))\n",
    "\n",
    "# Generating the initial distributions for spherical gaussian\n",
    "spherical_gaussian_distributions_1 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 1)\n",
    "spherical_gaussian_distributions_2 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 2)\n",
    "spherical_gaussian_distributions_3 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 3)\n",
    "\n",
    "# Generating the initial distributions for diagonal gaussian\n",
    "diagonal_gaussian_distributions_1 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 1)\n",
    "diagonal_gaussian_distributions_2 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 2)\n",
    "diagonal_gaussian_distributions_3 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8438357955288273\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_1, pi, k, threshold = threshold)\n",
    "\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8439338475617728\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_2, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running the expectation maximization algorithm for spherical gaussian\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions, iterations, diff \u001b[38;5;241m=\u001b[39m \u001b[43mexpectation_maximization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mspherical_gaussian_distributions_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m get_accuracy(y_train, predictions)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mexpectation_maximization\u001b[0;34m(X, normal_distributions, pi, k, threshold, maxiters)\u001b[0m\n\u001b[1;32m     32\u001b[0m normal_distributions \u001b[38;5;241m=\u001b[39m generate_data(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], k, mean \u001b[38;5;241m=\u001b[39m means, cov \u001b[38;5;241m=\u001b[39m covs, init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Calculating the value of log likelihood\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# and the cluster values for every x_i with t+1^th iteration mean and covariance\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m t_ll \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Calculating the difference of likelihood\u001b[39;00m\n\u001b[1;32m     39\u001b[0m t_diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(t_ll \u001b[38;5;241m-\u001b[39m t_1_ll)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mcalc_log_likelihood\u001b[0;34m(X, initial_distributions, pi, clusters)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_log_likelihood\u001b[39m(X, initial_distributions,pi, clusters):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Summary\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Calculates the log likelihood of the data given the initial distributions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        np.float64: Log likelihood of the data given the initial distributions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum([\n\u001b[1;32m     15\u001b[0m         initial_distributions[i]\u001b[38;5;241m.\u001b[39mpdf(X)\u001b[38;5;241m*\u001b[39m pi[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clusters)\n\u001b[1;32m     16\u001b[0m     ])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlog(values)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_log_likelihood\u001b[39m(X, initial_distributions,pi, clusters):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Summary\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Calculates the log likelihood of the data given the initial distributions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        np.float64: Log likelihood of the data given the initial distributions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum([\n\u001b[0;32m---> 15\u001b[0m         \u001b[43minitial_distributions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m pi[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clusters)\n\u001b[1;32m     16\u001b[0m     ])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlog(values)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/stats/_multivariate.py:750\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.pdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/stats/_multivariate.py:745\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.logpdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    744\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist\u001b[38;5;241m.\u001b[39m_process_quantiles(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[0;32m--> 745\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_pdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _squeeze_output(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/stats/_multivariate.py:470\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._logpdf\u001b[0;34m(self, x, mean, prec_U, log_det_cov, rank)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m dev \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m mean\n\u001b[0;32m--> 470\u001b[0m maha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_U\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (rank \u001b[38;5;241m*\u001b[39m _LOG_2PI \u001b[38;5;241m+\u001b[39m log_det_cov \u001b[38;5;241m+\u001b[39m maha)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_3, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_1, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_2, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_3, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2db188c233a700111d30e3ccbeb8d5ce15dbfb83d194c6870e309733805c539"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
